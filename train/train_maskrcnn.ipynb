{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "trainmaskrcnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "89b858138fe64eea87c67525f605204e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8b70e53ce80a4a2daa948130866b2793",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1b22624b8f154496a61a4240bb72086d",
              "IPY_MODEL_6c72b8e511e84d56bee5c7ed5850cbb3"
            ]
          }
        },
        "8b70e53ce80a4a2daa948130866b2793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b22624b8f154496a61a4240bb72086d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3bd8339b9fdc46968639bcb10706b7be",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01b5da581e7d4e63b65c480ebc82cddf"
          }
        },
        "6c72b8e511e84d56bee5c7ed5850cbb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_000d107fd2084d35b195bcc0aacb22f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 146MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_beddfb98e64b4d3cab2de65cee08307d"
          }
        },
        "3bd8339b9fdc46968639bcb10706b7be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01b5da581e7d4e63b65c480ebc82cddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "000d107fd2084d35b195bcc0aacb22f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "beddfb98e64b4d3cab2de65cee08307d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_kL-50oGgk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, models, transforms\n",
        "from PIL import Image\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "import copy\n",
        "import math\n",
        "import itertools as it\n",
        "from torch.optim import Optimizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8AuJ-l9IHmp",
        "colab_type": "text"
      },
      "source": [
        "###Defining the dataset \n",
        "\n",
        "* image: a PIL Image of size (H, W)\n",
        "* target: a dict containing the following fields\n",
        "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
        "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
        "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset.\n",
        "    * `area` (`Tensor[N]`): The area of the bounding box.\n",
        "    * `iscrowd` (`UInt8Tensor[N]`): boolean representation of whether the image has crowded instances\n",
        "    * (optionally) `masks` (`UInt8Tensor[N, H, W]`): The segmentation masks for each one of the objects\n",
        "    * (optionally) `keypoints` (`FloatTensor[N, K, 3]`): For each one of the `N` objects, it contains the `K` keypoints in `[x, y, visibility]` format, defining the object. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNbCLjSHILVd",
        "colab_type": "text"
      },
      "source": [
        "### Writing a custom dataset for Penn-Fudan\n",
        "\n",
        " Write a dataset for the Penn-Fudan dataset.\n",
        "\n",
        "First, download and extract the data, present in a zip file at https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPy4USLd77W_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a287376d-40f3-43f4-8a0b-ff92467522bb"
      },
      "source": [
        "# # Run shell to download the dataset and download the Penn-Fudan dataset\n",
        "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
        "# # extract it in the current folder\n",
        "!unzip PennFudanPed.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-28 15:23:59--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
            "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n",
            "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53723336 (51M) [application/zip]\n",
            "Saving to: â€˜PennFudanPed.zipâ€™\n",
            "\n",
            "PennFudanPed.zip    100%[===================>]  51.23M  1009KB/s    in 49s     \n",
            "\n",
            "2020-06-28 15:24:49 (1.04 MB/s) - â€˜PennFudanPed.zipâ€™ saved [53723336/53723336]\n",
            "\n",
            "--2020-06-28 15:24:49--  http://./\n",
            "Resolving . (.)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address â€˜.â€™\n",
            "FINISHED --2020-06-28 15:24:49--\n",
            "Total wall clock time: 50s\n",
            "Downloaded: 1 files, 51M in 49s (1.04 MB/s)\n",
            "Archive:  PennFudanPed.zip\n",
            "   creating: PennFudanPed/\n",
            "  inflating: PennFudanPed/added-object-list.txt  \n",
            "   creating: PennFudanPed/Annotation/\n",
            "  inflating: PennFudanPed/Annotation/FudanPed00001.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00002.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00003.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00004.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00005.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00006.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00007.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00008.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00009.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00010.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00011.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00012.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00013.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00014.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00015.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00016.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00017.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00018.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00019.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00020.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00021.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00022.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00023.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00024.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00025.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00026.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00027.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00028.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00029.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00030.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00031.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00032.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00033.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00034.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00035.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00036.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00037.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00038.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00039.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00040.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00041.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00042.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00043.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00044.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00045.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00046.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00047.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00048.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00049.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00050.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00051.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00052.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00053.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00054.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00055.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00056.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00057.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00058.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00059.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00060.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00061.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00062.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00063.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00064.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00065.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00066.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00067.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00068.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00069.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00070.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00071.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00072.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00073.txt  \n",
            "  inflating: PennFudanPed/Annotation/FudanPed00074.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00001.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00002.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00003.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00004.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00005.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00006.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00007.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00008.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00009.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00010.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00011.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00012.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00013.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00014.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00015.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00016.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00017.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00018.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00019.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00020.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00021.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00022.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00023.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00024.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00025.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00026.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00027.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00028.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00029.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00030.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00031.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00032.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00033.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00034.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00035.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00036.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00037.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00038.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00039.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00040.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00041.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00042.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00043.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00044.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00045.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00046.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00047.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00048.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00049.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00050.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00051.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00052.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00053.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00054.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00055.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00056.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00057.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00058.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00059.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00060.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00061.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00062.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00063.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00064.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00065.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00066.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00067.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00068.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00069.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00070.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00071.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00072.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00073.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00074.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00075.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00076.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00077.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00078.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00079.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00080.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00081.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00082.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00083.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00084.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00085.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00086.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00087.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00088.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00089.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00090.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00091.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00092.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00093.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00094.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00095.txt  \n",
            "  inflating: PennFudanPed/Annotation/PennPed00096.txt  \n",
            "   creating: PennFudanPed/PedMasks/\n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00001_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00002_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00003_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00004_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00005_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00006_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00007_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00008_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00009_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00010_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00011_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00012_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00013_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00014_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/FudanPed00015_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00016_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00017_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/FudanPed00018_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00019_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00020_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00021_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00022_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00023_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00024_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00025_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00026_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00027_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/FudanPed00028_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00029_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00030_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00031_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00032_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00033_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00034_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00035_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00036_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00037_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00038_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00039_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00040_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00041_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00042_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00043_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00044_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00045_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00046_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00047_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00048_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00049_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00050_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00051_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00052_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00053_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00054_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00055_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00056_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00057_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00058_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00059_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00060_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00061_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00062_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00063_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00064_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00065_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00066_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00067_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00068_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00069_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00070_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00071_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00072_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00073_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/FudanPed00074_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00001_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00002_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00003_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00004_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00005_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00006_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00007_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00008_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00009_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00010_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00011_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00012_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00013_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00014_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00015_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00016_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00017_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00018_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00019_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00020_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00021_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00022_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00023_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00024_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00025_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00026_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00027_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00028_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00029_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00030_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00031_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00032_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00033_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00034_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00035_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00036_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00037_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00038_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00039_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00040_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00041_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00042_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00043_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00044_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00045_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00046_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00047_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00048_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00049_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00050_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00051_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00052_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00053_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00054_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00055_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00056_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00057_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00058_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00059_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00060_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00061_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00062_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00063_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00064_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00065_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00066_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00067_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00068_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00069_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00070_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00071_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00072_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00073_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00074_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00075_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00076_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00077_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00078_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00079_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00080_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00081_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00082_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00083_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00084_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00085_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00086_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00087_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00088_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00089_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00090_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00091_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00092_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00093_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00094_mask.png  \n",
            "  inflating: PennFudanPed/PedMasks/PennPed00095_mask.png  \n",
            " extracting: PennFudanPed/PedMasks/PennPed00096_mask.png  \n",
            "   creating: PennFudanPed/PNGImages/\n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00001.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00002.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00003.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00004.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00005.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00006.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00007.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00008.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00009.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00010.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00011.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00012.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00013.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00014.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00015.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00016.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00017.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00018.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00019.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00020.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00021.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00022.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00023.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00024.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00025.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00026.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00027.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00028.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00029.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00030.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00031.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00032.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00033.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00034.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00035.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00036.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00037.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00038.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00039.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00040.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00041.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00042.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00043.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00044.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00045.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00046.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00047.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00048.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00049.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00050.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00051.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00052.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00053.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00054.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00055.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00056.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00057.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00058.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00059.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00060.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00061.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00062.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00063.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00064.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00065.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00066.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00067.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00068.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00069.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00070.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00071.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00072.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00073.png  \n",
            "  inflating: PennFudanPed/PNGImages/FudanPed00074.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00001.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00002.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00003.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00004.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00005.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00006.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00007.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00008.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00009.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00010.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00011.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00012.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00013.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00014.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00015.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00016.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00017.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00018.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00019.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00020.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00021.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00022.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00023.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00024.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00025.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00026.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00027.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00028.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00029.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00030.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00031.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00032.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00033.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00034.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00035.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00036.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00037.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00038.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00039.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00040.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00041.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00042.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00043.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00044.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00045.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00046.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00047.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00048.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00049.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00050.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00051.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00052.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00053.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00054.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00055.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00056.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00057.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00058.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00059.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00060.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00061.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00062.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00063.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00064.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00065.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00066.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00067.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00068.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00069.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00070.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00071.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00072.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00073.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00074.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00075.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00076.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00077.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00078.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00079.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00080.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00081.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00082.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00083.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00084.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00085.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00086.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00087.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00088.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00089.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00090.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00091.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00092.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00093.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00094.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00095.png  \n",
            "  inflating: PennFudanPed/PNGImages/PennPed00096.png  \n",
            "  inflating: PennFudanPed/readme.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE57JlSjIXVn",
        "colab_type": "text"
      },
      "source": [
        "The data is structured as follows\n",
        "```\n",
        "PennFudanPed/\n",
        "  Annotations/\n",
        "    FudanPed00001.txt\n",
        "    FudanPed00002.txt\n",
        "    ...\n",
        "  PedMasks/\n",
        "    FudanPed00001_mask.png\n",
        "    FudanPed00002_mask.png\n",
        "    ...\n",
        "  PNGImages/\n",
        "    FudanPed00001.png\n",
        "    FudanPed00002.png\n",
        "    ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvSA_4GFIixr",
        "colab_type": "text"
      },
      "source": [
        "Write a `torch.utils.data.Dataset` class for this dataset for the input image and target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdPyb1dc77XH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Dataset class\n",
        "class PennFudanDataset(data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks.astype(np.uint8), dtype=torch.uint8)\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql-HvuGsIvVZ",
        "colab_type": "text"
      },
      "source": [
        "That's all for the dataset. The structure of the output dictionary is shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Vk_-kFIx9Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90169634-1180-4b75-dc4e-05b402a81a2c"
      },
      "source": [
        "dataset = PennFudanDataset('PennFudanPed/')\n",
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=559x536 at 0x7FC6C869C6D8>,\n",
              " {'area': tensor([35358., 36225.]), 'boxes': tensor([[159., 181., 301., 430.],\n",
              "          [419., 170., 534., 485.]]), 'image_id': tensor([0]), 'iscrowd': tensor([0, 0]), 'labels': tensor([1, 1]), 'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           ...,\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0]],\n",
              "  \n",
              "          [[0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           ...,\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb9YWZtVI2_n",
        "colab_type": "text"
      },
      "source": [
        "### Model\n",
        "\n",
        "I'm using Maskrcnn model, provided by torchvision. The backbone used is `resnet_50_fpn`. The model is further modified for our use case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOnBVNQIJdO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function for model\n",
        "def get_backbone(num_classes):\n",
        "  backbone = torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "  new_backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
        "  new_backbone.out_channels = 2048\n",
        "\n",
        "  model = MaskRCNN(new_backbone, num_classes)\n",
        "  return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd2gDBZPNyiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model\n",
        "def get_instance_segmentation_model(num_classes):\n",
        "    \n",
        "    # get the maskrcnn model\n",
        "    model = get_backbone(num_classes)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxgElV-QJzLX",
        "colab_type": "text"
      },
      "source": [
        "Copy files from [pytorch github](https://github.com/pytorch/vision.git) to access helper functions, used for training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiCu4nLzgPb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "54fc6d01-36ff-48b0-9c81-5e8a63a37a47"
      },
      "source": [
        "%%shell\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 8631 (delta 32), reused 18 (delta 5), pack-reused 8562\u001b[K\n",
            "Receiving objects: 100% (8631/8631), 10.38 MiB | 6.86 MiB/s, done.\n",
            "Resolving deltas: 100% (5949/5949), done.\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be37608 version check against PyTorch's CUDA version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov9N48-sKG_j",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Write a helper function for data augmentation, which leverages the functions in `refereces/detection` that we have just copied:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GzUc8bXKJHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXdGr5UDKnYN",
        "colab_type": "text"
      },
      "source": [
        "Split the data to training and validation sets and create pytorch dataloader objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWaQV1sg77Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine X and y within a batch\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE8BBl9k_V-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "train_dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "val_dataset = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(train_dataset)).tolist()\n",
        "train_dataset = torch.utils.data.Subset(train_dataset, indices[:-50])\n",
        "val_dataset = torch.utils.data.Subset(val_dataset, indices[-50:])\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "# define training and validation data loaders\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=1, shuffle=False,\n",
        "    collate_fn=collate_fn)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRggWakCMkjA",
        "colab_type": "text"
      },
      "source": [
        "Length of training set : 120\n",
        "\n",
        "Length of validation set : 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgRrU744NfCI",
        "colab_type": "text"
      },
      "source": [
        "Instantiate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYweEzSUVM1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "89b858138fe64eea87c67525f605204e",
            "8b70e53ce80a4a2daa948130866b2793",
            "1b22624b8f154496a61a4240bb72086d",
            "6c72b8e511e84d56bee5c7ed5850cbb3",
            "3bd8339b9fdc46968639bcb10706b7be",
            "01b5da581e7d4e63b65c480ebc82cddf",
            "000d107fd2084d35b195bcc0aacb22f2",
            "beddfb98e64b4d3cab2de65cee08307d"
          ]
        },
        "outputId": "0f500157-6f06-47a8-b843-868222359768"
      },
      "source": [
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# get the model\n",
        "model = get_instance_segmentation_model(num_classes)\n",
        "model = model.to(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89b858138fe64eea87c67525f605204e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfxYYhitYo6b",
        "colab_type": "text"
      },
      "source": [
        "Define optimizer and sceduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7uKo-oaYLBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define our optimizer classes- RAdam and LookAhead\n",
        "class Lookahead(Optimizer):\n",
        "    def __init__(self, base_optimizer,alpha=0.5, k=6):\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "        self.optimizer = base_optimizer\n",
        "        self.param_groups = self.optimizer.param_groups\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "        for group in self.param_groups:\n",
        "            group[\"step_counter\"] = 0\n",
        "        self.slow_weights = [[p.clone().detach() for p in group['params']]\n",
        "                                for group in self.param_groups]\n",
        "\n",
        "        for w in it.chain(*self.slow_weights):\n",
        "            w.requires_grad = False\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "        loss = self.optimizer.step()\n",
        "        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n",
        "            group['step_counter'] += 1\n",
        "            if group['step_counter'] % self.k != 0:\n",
        "                continue\n",
        "            for p,q in zip(group['params'],slow_weights):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                q.data.add_(self.alpha,p.data - q.data)\n",
        "                p.data.copy_(q.data)\n",
        "        return loss\n",
        "\n",
        "class RAdam(Optimizer):\n",
        "    '''\n",
        "    a PyTorch implementation of the RAdam Optimizer from th paper\n",
        "    On the Variance of the Adaptive Learning Rate and Beyond.\n",
        "\n",
        "    https://arxiv.org/abs/1908.03265\n",
        "    Example:\n",
        "        >>> from optimizer import RAdam\n",
        "        >>> optimizer = RAdam(model.parameters(), lr=0.001)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = self.buffer[int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > 5:\n",
        "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                if N_sma > 5:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "#\n",
        "class Ralamb(Optimizer):\n",
        "    '''\n",
        "    Ralamb optimizer (RAdam + LARS trick)\n",
        "    '''\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(Ralamb, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Ralamb, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Ralamb does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # m_t\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                # v_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = self.buffer[int(state['step'] % 10)]\n",
        "\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, radam_step = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        radam_step = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        radam_step = group['lr'] / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = radam_step\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
        "                radam_norm = p_data_fp32.pow(2).sum().sqrt()\n",
        "                if weight_norm == 0 or radam_norm == 0:\n",
        "                    trust_ratio = 1\n",
        "                else:\n",
        "                    trust_ratio = weight_norm / radam_norm\n",
        "\n",
        "                state['weight_norm'] = weight_norm\n",
        "                state['adam_norm'] = radam_norm\n",
        "                state['trust_ratio'] = trust_ratio\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-radam_step * trust_ratio, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-radam_step * trust_ratio, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kli9oRcJYhpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define optimizer and scheduler \n",
        "base_optim = RAdam(model.parameters(), lr = 0.001) \n",
        "optimizer =  Lookahead(base_optim, k=5, alpha=0.5)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=20,  gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSVz0dojVTUc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94466bde-a761-4559-f6a8-489d2d4cb32c"
      },
      "source": [
        "#resnet_50 parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('Numer of trainable parameters : {}'.format(count_parameters(model)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numer of trainable parameters : 171862875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtpBbllANuVL",
        "colab_type": "text"
      },
      "source": [
        "Helper function to calculate IoU for predictions on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd805r1nQ0lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_iou(pred_masks, target_masks):\n",
        "    target_mask_comb = np.sum(target_masks, axis=0)\n",
        "    pred_mask_comb = np.sum(pred_masks, axis=0)\n",
        "    \n",
        "    mask = np.add(pred_mask_comb,target_mask_comb)\n",
        "    \n",
        "    union = np.sum(mask>0)\n",
        "    inter = np.sum(mask==2)\n",
        "    \n",
        "    IoU = inter/union\n",
        "\n",
        "    return IoU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6szpO0iN4Mo",
        "colab_type": "text"
      },
      "source": [
        "Train the model for 70 epochs and save weights for the highest IoU on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icj8VVYfZcQo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f80d5901-fea9-4acd-8dbc-de72bdcaab10"
      },
      "source": [
        "# training cell\n",
        "BATCH_SIZE=2\n",
        "N_EPOCH = 70\n",
        "\n",
        "IoU=0\n",
        "for epoch in range(N_EPOCH):\n",
        "\n",
        "  # use helper function to train model\n",
        "  train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=61)\n",
        "\n",
        "  # update the learning rate \n",
        "  scheduler.step()\n",
        "\n",
        "  # evaluate on the validation set\n",
        "  model.eval()\n",
        "  curr_IoU = 0\n",
        "  with torch.no_grad():\n",
        "    for images, targets in val_dataloader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # get predictions\n",
        "        pred = model(images)\n",
        "\n",
        "        # post-processing to get masks with scores>0.5\n",
        "        pred_masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
        "        pred_score = list(pred[0]['scores'].detach().cpu().numpy())\n",
        "        pred_t = [pred_score.index(x) for x in pred_score if x>0.5]\n",
        "        if len(pred_t) == 0:\n",
        "          continue\n",
        "        pred_t = pred_t[-1]\n",
        "\n",
        "        pred_masks = pred_masks[:pred_t+1]\n",
        "        target_masks = targets[0]['masks'].detach().cpu().numpy()\n",
        "\n",
        "        # calculate IoU for the current batch\n",
        "        batch_IoU = calc_iou(pred_masks, target_masks)\n",
        "        curr_IoU+=batch_IoU\n",
        "\n",
        "  print(\"Epoch: {} Validation IoU: {}\"\n",
        "  .format(epoch, curr_IoU/len(val_dataset)))\n",
        "  if curr_IoU>IoU:\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      IoU = curr_IoU\n",
        "      print('Model saved in {} epoch'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0]  [ 0/60]  eta: 0:01:50  lr: 0.000018  loss: 12.6068 (12.6068)  loss_classifier: 0.7011 (0.7011)  loss_box_reg: 0.0001 (0.0001)  loss_mask: 6.3443 (6.3443)  loss_objectness: 0.6883 (0.6883)  loss_rpn_box_reg: 4.8731 (4.8731)  time: 1.8386  data: 0.1575  max mem: 9082\n",
            "Epoch: [0]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 3.3934 (6.5869)  loss_classifier: 0.0914 (0.2226)  loss_box_reg: 0.0569 (0.0507)  loss_mask: 0.6566 (2.5259)  loss_objectness: 0.1896 (0.3901)  loss_rpn_box_reg: 2.2654 (3.3976)  time: 1.9057  data: 0.0907  max mem: 9082\n",
            "Epoch: [0] Total time: 0:01:52 (1.8702 s / it)\n",
            "Epoch: 0 Validation IoU: 0.0\n",
            "Epoch: [1]  [ 0/60]  eta: 0:02:12  lr: 0.001000  loss: 2.4728 (2.4728)  loss_classifier: 0.1152 (0.1152)  loss_box_reg: 0.0800 (0.0800)  loss_mask: 0.6587 (0.6587)  loss_objectness: 0.1163 (0.1163)  loss_rpn_box_reg: 1.5026 (1.5026)  time: 2.2123  data: 0.1866  max mem: 9082\n",
            "Epoch: [1]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 2.0038 (2.2576)  loss_classifier: 0.0357 (0.0521)  loss_box_reg: 0.0393 (0.0567)  loss_mask: 0.3406 (0.4047)  loss_objectness: 0.1163 (0.1451)  loss_rpn_box_reg: 1.3801 (1.5990)  time: 1.9070  data: 0.0788  max mem: 9082\n",
            "Epoch: [1] Total time: 0:01:54 (1.9145 s / it)\n",
            "Epoch: 1 Validation IoU: 0.42350811748514444\n",
            "Model saved in 1 epoch\n",
            "Epoch: [2]  [ 0/60]  eta: 0:01:57  lr: 0.001000  loss: 1.8759 (1.8759)  loss_classifier: 0.0386 (0.0386)  loss_box_reg: 0.0686 (0.0686)  loss_mask: 0.3909 (0.3909)  loss_objectness: 0.1255 (0.1255)  loss_rpn_box_reg: 1.2523 (1.2523)  time: 1.9610  data: 0.1115  max mem: 9082\n",
            "Epoch: [2]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 1.7283 (1.7933)  loss_classifier: 0.0343 (0.0379)  loss_box_reg: 0.0569 (0.0501)  loss_mask: 0.3358 (0.3431)  loss_objectness: 0.1162 (0.1220)  loss_rpn_box_reg: 1.2093 (1.2402)  time: 1.9447  data: 0.0863  max mem: 9218\n",
            "Epoch: [2] Total time: 0:01:54 (1.9059 s / it)\n",
            "Epoch: 2 Validation IoU: 0.5470801604342291\n",
            "Model saved in 2 epoch\n",
            "Epoch: [3]  [ 0/60]  eta: 0:01:52  lr: 0.001000  loss: 1.4801 (1.4801)  loss_classifier: 0.0339 (0.0339)  loss_box_reg: 0.0318 (0.0318)  loss_mask: 0.2820 (0.2820)  loss_objectness: 0.1618 (0.1618)  loss_rpn_box_reg: 0.9705 (0.9705)  time: 1.8772  data: 0.1100  max mem: 9218\n",
            "Epoch: [3]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 1.5259 (1.5689)  loss_classifier: 0.0245 (0.0334)  loss_box_reg: 0.0291 (0.0445)  loss_mask: 0.2595 (0.2961)  loss_objectness: 0.1042 (0.1105)  loss_rpn_box_reg: 1.0534 (1.0844)  time: 1.8673  data: 0.0722  max mem: 9599\n",
            "Epoch: [3] Total time: 0:01:53 (1.8980 s / it)\n",
            "Epoch: 3 Validation IoU: 0.5023482582030678\n",
            "Epoch: [4]  [ 0/60]  eta: 0:02:08  lr: 0.001000  loss: 1.2262 (1.2262)  loss_classifier: 0.0272 (0.0272)  loss_box_reg: 0.0342 (0.0342)  loss_mask: 0.3133 (0.3133)  loss_objectness: 0.0837 (0.0837)  loss_rpn_box_reg: 0.7678 (0.7678)  time: 2.1446  data: 0.1063  max mem: 9599\n",
            "Epoch: [4]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 1.3582 (1.4399)  loss_classifier: 0.0266 (0.0284)  loss_box_reg: 0.0297 (0.0346)  loss_mask: 0.2609 (0.2717)  loss_objectness: 0.0796 (0.1044)  loss_rpn_box_reg: 0.9405 (1.0008)  time: 1.9065  data: 0.0861  max mem: 9599\n",
            "Epoch: [4] Total time: 0:01:53 (1.8944 s / it)\n",
            "Epoch: 4 Validation IoU: 0.6208587869021998\n",
            "Model saved in 4 epoch\n",
            "Epoch: [5]  [ 0/60]  eta: 0:02:06  lr: 0.001000  loss: 1.1175 (1.1175)  loss_classifier: 0.0170 (0.0170)  loss_box_reg: 0.0309 (0.0309)  loss_mask: 0.2223 (0.2223)  loss_objectness: 0.1170 (0.1170)  loss_rpn_box_reg: 0.7304 (0.7304)  time: 2.1094  data: 0.1577  max mem: 9599\n",
            "Epoch: [5]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 1.2044 (1.3207)  loss_classifier: 0.0212 (0.0266)  loss_box_reg: 0.0217 (0.0292)  loss_mask: 0.2436 (0.2452)  loss_objectness: 0.0811 (0.0887)  loss_rpn_box_reg: 0.8483 (0.9309)  time: 1.9392  data: 0.0983  max mem: 9599\n",
            "Epoch: [5] Total time: 0:01:55 (1.9232 s / it)\n",
            "Epoch: 5 Validation IoU: 0.5570827708190597\n",
            "Epoch: [6]  [ 0/60]  eta: 0:02:06  lr: 0.001000  loss: 1.1299 (1.1299)  loss_classifier: 0.0232 (0.0232)  loss_box_reg: 0.0234 (0.0234)  loss_mask: 0.2843 (0.2843)  loss_objectness: 0.0988 (0.0988)  loss_rpn_box_reg: 0.7002 (0.7002)  time: 2.1060  data: 0.0570  max mem: 9599\n",
            "Epoch: [6]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 1.2994 (1.2657)  loss_classifier: 0.0195 (0.0237)  loss_box_reg: 0.0249 (0.0274)  loss_mask: 0.2023 (0.2291)  loss_objectness: 0.0790 (0.0851)  loss_rpn_box_reg: 0.9328 (0.9004)  time: 1.8510  data: 0.0759  max mem: 9753\n",
            "Epoch: [6] Total time: 0:01:54 (1.9115 s / it)\n",
            "Epoch: 6 Validation IoU: 0.6050781934119914\n",
            "Epoch: [7]  [ 0/60]  eta: 0:01:47  lr: 0.001000  loss: 1.1348 (1.1348)  loss_classifier: 0.0145 (0.0145)  loss_box_reg: 0.0190 (0.0190)  loss_mask: 0.2491 (0.2491)  loss_objectness: 0.0746 (0.0746)  loss_rpn_box_reg: 0.7775 (0.7775)  time: 1.7998  data: 0.1287  max mem: 9753\n",
            "Epoch: [7]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 1.1594 (1.1959)  loss_classifier: 0.0332 (0.0283)  loss_box_reg: 0.0293 (0.0309)  loss_mask: 0.2094 (0.2226)  loss_objectness: 0.0882 (0.0834)  loss_rpn_box_reg: 0.7930 (0.8306)  time: 2.0290  data: 0.1122  max mem: 9753\n",
            "Epoch: [7] Total time: 0:01:56 (1.9367 s / it)\n",
            "Epoch: 7 Validation IoU: 0.6127466755825413\n",
            "Epoch: [8]  [ 0/60]  eta: 0:01:49  lr: 0.001000  loss: 1.1850 (1.1850)  loss_classifier: 0.0178 (0.0178)  loss_box_reg: 0.0210 (0.0210)  loss_mask: 0.2215 (0.2215)  loss_objectness: 0.0967 (0.0967)  loss_rpn_box_reg: 0.8281 (0.8281)  time: 1.8309  data: 0.0395  max mem: 9753\n",
            "Epoch: [8]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 1.2898 (1.1886)  loss_classifier: 0.0161 (0.0240)  loss_box_reg: 0.0211 (0.0257)  loss_mask: 0.2239 (0.2246)  loss_objectness: 0.0772 (0.0811)  loss_rpn_box_reg: 0.8290 (0.8332)  time: 1.8649  data: 0.0665  max mem: 9753\n",
            "Epoch: [8] Total time: 0:01:53 (1.8941 s / it)\n",
            "Epoch: 8 Validation IoU: 0.600067537277122\n",
            "Epoch: [9]  [ 0/60]  eta: 0:01:41  lr: 0.001000  loss: 0.8048 (0.8048)  loss_classifier: 0.0094 (0.0094)  loss_box_reg: 0.0044 (0.0044)  loss_mask: 0.1562 (0.1562)  loss_objectness: 0.0294 (0.0294)  loss_rpn_box_reg: 0.6053 (0.6053)  time: 1.6953  data: 0.0639  max mem: 9753\n",
            "Epoch: [9]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 1.0083 (1.0895)  loss_classifier: 0.0215 (0.0226)  loss_box_reg: 0.0176 (0.0250)  loss_mask: 0.1770 (0.2066)  loss_objectness: 0.0641 (0.0796)  loss_rpn_box_reg: 0.6872 (0.7558)  time: 1.9215  data: 0.0912  max mem: 9753\n",
            "Epoch: [9] Total time: 0:01:54 (1.9069 s / it)\n",
            "Epoch: 9 Validation IoU: 0.6951765062337913\n",
            "Model saved in 9 epoch\n",
            "Epoch: [10]  [ 0/60]  eta: 0:01:56  lr: 0.001000  loss: 1.0448 (1.0448)  loss_classifier: 0.0091 (0.0091)  loss_box_reg: 0.0051 (0.0051)  loss_mask: 0.1748 (0.1748)  loss_objectness: 0.0492 (0.0492)  loss_rpn_box_reg: 0.8066 (0.8066)  time: 1.9476  data: 0.1027  max mem: 9753\n",
            "Epoch: [10]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 1.0879 (1.0679)  loss_classifier: 0.0191 (0.0180)  loss_box_reg: 0.0267 (0.0218)  loss_mask: 0.1817 (0.1909)  loss_objectness: 0.0598 (0.0685)  loss_rpn_box_reg: 0.7884 (0.7686)  time: 1.8924  data: 0.0912  max mem: 9852\n",
            "Epoch: [10] Total time: 0:01:54 (1.9077 s / it)\n",
            "Epoch: 10 Validation IoU: 0.6547371027092417\n",
            "Epoch: [11]  [ 0/60]  eta: 0:02:13  lr: 0.001000  loss: 1.1505 (1.1505)  loss_classifier: 0.0210 (0.0210)  loss_box_reg: 0.0235 (0.0235)  loss_mask: 0.2010 (0.2010)  loss_objectness: 0.0441 (0.0441)  loss_rpn_box_reg: 0.8609 (0.8609)  time: 2.2318  data: 0.1304  max mem: 9852\n",
            "Epoch: [11]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 0.9382 (1.0042)  loss_classifier: 0.0148 (0.0182)  loss_box_reg: 0.0199 (0.0190)  loss_mask: 0.1789 (0.1869)  loss_objectness: 0.0733 (0.0707)  loss_rpn_box_reg: 0.6671 (0.7094)  time: 1.9438  data: 0.0904  max mem: 9852\n",
            "Epoch: [11] Total time: 0:01:54 (1.9023 s / it)\n",
            "Epoch: 11 Validation IoU: 0.5834396169207686\n",
            "Epoch: [12]  [ 0/60]  eta: 0:01:41  lr: 0.001000  loss: 1.0357 (1.0357)  loss_classifier: 0.0183 (0.0183)  loss_box_reg: 0.0281 (0.0281)  loss_mask: 0.3100 (0.3100)  loss_objectness: 0.0598 (0.0598)  loss_rpn_box_reg: 0.6195 (0.6195)  time: 1.6912  data: 0.0248  max mem: 9852\n",
            "Epoch: [12]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 0.9646 (0.9481)  loss_classifier: 0.0252 (0.0198)  loss_box_reg: 0.0173 (0.0188)  loss_mask: 0.1842 (0.1903)  loss_objectness: 0.0616 (0.0602)  loss_rpn_box_reg: 0.6872 (0.6591)  time: 1.8941  data: 0.0824  max mem: 9852\n",
            "Epoch: [12] Total time: 0:01:54 (1.9121 s / it)\n",
            "Epoch: 12 Validation IoU: 0.546550533986891\n",
            "Epoch: [13]  [ 0/60]  eta: 0:01:43  lr: 0.001000  loss: 1.0305 (1.0305)  loss_classifier: 0.0298 (0.0298)  loss_box_reg: 0.0211 (0.0211)  loss_mask: 0.2239 (0.2239)  loss_objectness: 0.0622 (0.0622)  loss_rpn_box_reg: 0.6935 (0.6935)  time: 1.7264  data: 0.0705  max mem: 9852\n",
            "Epoch: [13]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 0.9755 (0.9600)  loss_classifier: 0.0128 (0.0215)  loss_box_reg: 0.0130 (0.0187)  loss_mask: 0.1817 (0.1834)  loss_objectness: 0.0595 (0.0605)  loss_rpn_box_reg: 0.6529 (0.6759)  time: 1.9284  data: 0.0784  max mem: 9852\n",
            "Epoch: [13] Total time: 0:01:54 (1.9072 s / it)\n",
            "Epoch: 13 Validation IoU: 0.5965460264802487\n",
            "Epoch: [14]  [ 0/60]  eta: 0:01:51  lr: 0.001000  loss: 0.9386 (0.9386)  loss_classifier: 0.0213 (0.0213)  loss_box_reg: 0.0150 (0.0150)  loss_mask: 0.1950 (0.1950)  loss_objectness: 0.0456 (0.0456)  loss_rpn_box_reg: 0.6618 (0.6618)  time: 1.8634  data: 0.0723  max mem: 9852\n",
            "Epoch: [14]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 0.8208 (0.9058)  loss_classifier: 0.0116 (0.0174)  loss_box_reg: 0.0083 (0.0172)  loss_mask: 0.1483 (0.1696)  loss_objectness: 0.0574 (0.0645)  loss_rpn_box_reg: 0.5804 (0.6372)  time: 1.8399  data: 0.0647  max mem: 9852\n",
            "Epoch: [14] Total time: 0:01:53 (1.8979 s / it)\n",
            "Epoch: 14 Validation IoU: 0.604760242367511\n",
            "Epoch: [15]  [ 0/60]  eta: 0:01:42  lr: 0.001000  loss: 0.5423 (0.5423)  loss_classifier: 0.0035 (0.0035)  loss_box_reg: 0.0026 (0.0026)  loss_mask: 0.1217 (0.1217)  loss_objectness: 0.0376 (0.0376)  loss_rpn_box_reg: 0.3769 (0.3769)  time: 1.7133  data: 0.1041  max mem: 9852\n",
            "Epoch: [15]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 0.9583 (0.8849)  loss_classifier: 0.0154 (0.0150)  loss_box_reg: 0.0123 (0.0150)  loss_mask: 0.1583 (0.1719)  loss_objectness: 0.0506 (0.0558)  loss_rpn_box_reg: 0.7081 (0.6272)  time: 1.9308  data: 0.0931  max mem: 9852\n",
            "Epoch: [15] Total time: 0:01:54 (1.9056 s / it)\n",
            "Epoch: 15 Validation IoU: 0.6029539664190776\n",
            "Epoch: [16]  [ 0/60]  eta: 0:01:44  lr: 0.001000  loss: 0.5186 (0.5186)  loss_classifier: 0.0026 (0.0026)  loss_box_reg: 0.0035 (0.0035)  loss_mask: 0.1156 (0.1156)  loss_objectness: 0.0431 (0.0431)  loss_rpn_box_reg: 0.3538 (0.3538)  time: 1.7491  data: 0.1208  max mem: 9852\n",
            "Epoch: [16]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 0.9179 (0.8919)  loss_classifier: 0.0115 (0.0157)  loss_box_reg: 0.0128 (0.0169)  loss_mask: 0.1437 (0.1639)  loss_objectness: 0.0425 (0.0514)  loss_rpn_box_reg: 0.6786 (0.6439)  time: 1.8548  data: 0.0911  max mem: 9852\n",
            "Epoch: [16] Total time: 0:01:53 (1.8917 s / it)\n",
            "Epoch: 16 Validation IoU: 0.5735388729452126\n",
            "Epoch: [17]  [ 0/60]  eta: 0:02:12  lr: 0.001000  loss: 0.7484 (0.7484)  loss_classifier: 0.0182 (0.0182)  loss_box_reg: 0.0139 (0.0139)  loss_mask: 0.1766 (0.1766)  loss_objectness: 0.0399 (0.0399)  loss_rpn_box_reg: 0.4998 (0.4998)  time: 2.2044  data: 0.1041  max mem: 9852\n",
            "Epoch: [17]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 0.8400 (0.8800)  loss_classifier: 0.0140 (0.0164)  loss_box_reg: 0.0123 (0.0182)  loss_mask: 0.1428 (0.1626)  loss_objectness: 0.0488 (0.0611)  loss_rpn_box_reg: 0.6174 (0.6218)  time: 1.8563  data: 0.0722  max mem: 9852\n",
            "Epoch: [17] Total time: 0:01:54 (1.9029 s / it)\n",
            "Epoch: 17 Validation IoU: 0.5905036833418432\n",
            "Epoch: [18]  [ 0/60]  eta: 0:01:45  lr: 0.001000  loss: 0.8125 (0.8125)  loss_classifier: 0.0144 (0.0144)  loss_box_reg: 0.0187 (0.0187)  loss_mask: 0.2453 (0.2453)  loss_objectness: 0.0204 (0.0204)  loss_rpn_box_reg: 0.5137 (0.5137)  time: 1.7519  data: 0.0384  max mem: 9852\n",
            "Epoch: [18]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 0.9409 (0.8580)  loss_classifier: 0.0234 (0.0225)  loss_box_reg: 0.0162 (0.0162)  loss_mask: 0.1600 (0.1667)  loss_objectness: 0.0552 (0.0568)  loss_rpn_box_reg: 0.5850 (0.5958)  time: 1.9255  data: 0.0716  max mem: 9852\n",
            "Epoch: [18] Total time: 0:01:52 (1.8768 s / it)\n",
            "Epoch: 18 Validation IoU: 0.6228541495329503\n",
            "Epoch: [19]  [ 0/60]  eta: 0:01:51  lr: 0.001000  loss: 0.8786 (0.8786)  loss_classifier: 0.0256 (0.0256)  loss_box_reg: 0.0127 (0.0127)  loss_mask: 0.1636 (0.1636)  loss_objectness: 0.0573 (0.0573)  loss_rpn_box_reg: 0.6194 (0.6194)  time: 1.8539  data: 0.0673  max mem: 9852\n",
            "Epoch: [19]  [59/60]  eta: 0:00:01  lr: 0.001000  loss: 0.7555 (0.8268)  loss_classifier: 0.0092 (0.0199)  loss_box_reg: 0.0086 (0.0153)  loss_mask: 0.1315 (0.1570)  loss_objectness: 0.0442 (0.0542)  loss_rpn_box_reg: 0.5170 (0.5803)  time: 1.9158  data: 0.0697  max mem: 9852\n",
            "Epoch: [19] Total time: 0:01:54 (1.9025 s / it)\n",
            "Epoch: 19 Validation IoU: 0.6032990070259691\n",
            "Epoch: [20]  [ 0/60]  eta: 0:01:47  lr: 0.000100  loss: 0.7806 (0.7806)  loss_classifier: 0.0070 (0.0070)  loss_box_reg: 0.0082 (0.0082)  loss_mask: 0.1355 (0.1355)  loss_objectness: 0.0562 (0.0562)  loss_rpn_box_reg: 0.5737 (0.5737)  time: 1.7912  data: 0.0211  max mem: 9852\n",
            "Epoch: [20]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.6309 (0.6451)  loss_classifier: 0.0064 (0.0086)  loss_box_reg: 0.0058 (0.0093)  loss_mask: 0.1162 (0.1432)  loss_objectness: 0.0332 (0.0464)  loss_rpn_box_reg: 0.4697 (0.4376)  time: 1.8715  data: 0.0707  max mem: 9852\n",
            "Epoch: [20] Total time: 0:01:52 (1.8809 s / it)\n",
            "Epoch: 20 Validation IoU: 0.5621034398465589\n",
            "Epoch: [21]  [ 0/60]  eta: 0:01:45  lr: 0.000100  loss: 0.6348 (0.6348)  loss_classifier: 0.0029 (0.0029)  loss_box_reg: 0.0109 (0.0109)  loss_mask: 0.1346 (0.1346)  loss_objectness: 0.0622 (0.0622)  loss_rpn_box_reg: 0.4243 (0.4243)  time: 1.7551  data: 0.0564  max mem: 9852\n",
            "Epoch: [21]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.5924 (0.5757)  loss_classifier: 0.0052 (0.0076)  loss_box_reg: 0.0064 (0.0075)  loss_mask: 0.1233 (0.1360)  loss_objectness: 0.0400 (0.0406)  loss_rpn_box_reg: 0.4056 (0.3840)  time: 1.8760  data: 0.0934  max mem: 9852\n",
            "Epoch: [21] Total time: 0:01:53 (1.8971 s / it)\n",
            "Epoch: 21 Validation IoU: 0.5422190300313621\n",
            "Epoch: [22]  [ 0/60]  eta: 0:01:52  lr: 0.000100  loss: 0.6546 (0.6546)  loss_classifier: 0.0050 (0.0050)  loss_box_reg: 0.0117 (0.0117)  loss_mask: 0.2111 (0.2111)  loss_objectness: 0.0251 (0.0251)  loss_rpn_box_reg: 0.4017 (0.4017)  time: 1.8755  data: 0.1112  max mem: 9852\n",
            "Epoch: [22]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.5391 (0.5306)  loss_classifier: 0.0070 (0.0070)  loss_box_reg: 0.0063 (0.0068)  loss_mask: 0.1242 (0.1354)  loss_objectness: 0.0346 (0.0380)  loss_rpn_box_reg: 0.3334 (0.3434)  time: 1.9250  data: 0.0801  max mem: 9852\n",
            "Epoch: [22] Total time: 0:01:53 (1.8837 s / it)\n",
            "Epoch: 22 Validation IoU: 0.5382345947846303\n",
            "Epoch: [23]  [ 0/60]  eta: 0:01:44  lr: 0.000100  loss: 0.7080 (0.7080)  loss_classifier: 0.0130 (0.0130)  loss_box_reg: 0.0174 (0.0174)  loss_mask: 0.1898 (0.1898)  loss_objectness: 0.0541 (0.0541)  loss_rpn_box_reg: 0.4337 (0.4337)  time: 1.7454  data: 0.0923  max mem: 9852\n",
            "Epoch: [23]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.5097 (0.5191)  loss_classifier: 0.0082 (0.0069)  loss_box_reg: 0.0043 (0.0066)  loss_mask: 0.1271 (0.1368)  loss_objectness: 0.0278 (0.0362)  loss_rpn_box_reg: 0.3088 (0.3325)  time: 1.8710  data: 0.0654  max mem: 9852\n",
            "Epoch: [23] Total time: 0:01:53 (1.8962 s / it)\n",
            "Epoch: 23 Validation IoU: 0.5583701150331857\n",
            "Epoch: [24]  [ 0/60]  eta: 0:01:46  lr: 0.000100  loss: 0.3531 (0.3531)  loss_classifier: 0.0039 (0.0039)  loss_box_reg: 0.0013 (0.0013)  loss_mask: 0.0896 (0.0896)  loss_objectness: 0.0486 (0.0486)  loss_rpn_box_reg: 0.2098 (0.2098)  time: 1.7749  data: 0.0858  max mem: 9852\n",
            "Epoch: [24]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.4706 (0.4912)  loss_classifier: 0.0051 (0.0075)  loss_box_reg: 0.0030 (0.0065)  loss_mask: 0.1078 (0.1323)  loss_objectness: 0.0322 (0.0347)  loss_rpn_box_reg: 0.3009 (0.3102)  time: 1.8146  data: 0.0633  max mem: 9852\n",
            "Epoch: [24] Total time: 0:01:53 (1.8975 s / it)\n",
            "Epoch: 24 Validation IoU: 0.5007953774414691\n",
            "Epoch: [25]  [ 0/60]  eta: 0:02:05  lr: 0.000100  loss: 0.4211 (0.4211)  loss_classifier: 0.0171 (0.0171)  loss_box_reg: 0.0114 (0.0114)  loss_mask: 0.1523 (0.1523)  loss_objectness: 0.0158 (0.0158)  loss_rpn_box_reg: 0.2246 (0.2246)  time: 2.0937  data: 0.0581  max mem: 9852\n",
            "Epoch: [25]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.4736 (0.4737)  loss_classifier: 0.0049 (0.0067)  loss_box_reg: 0.0050 (0.0061)  loss_mask: 0.1162 (0.1295)  loss_objectness: 0.0311 (0.0317)  loss_rpn_box_reg: 0.3110 (0.2997)  time: 1.8353  data: 0.0765  max mem: 9852\n",
            "Epoch: [25] Total time: 0:01:52 (1.8708 s / it)\n",
            "Epoch: 25 Validation IoU: 0.5469694815518313\n",
            "Epoch: [26]  [ 0/60]  eta: 0:01:45  lr: 0.000100  loss: 0.3906 (0.3906)  loss_classifier: 0.0067 (0.0067)  loss_box_reg: 0.0023 (0.0023)  loss_mask: 0.1078 (0.1078)  loss_objectness: 0.0227 (0.0227)  loss_rpn_box_reg: 0.2511 (0.2511)  time: 1.7614  data: 0.0295  max mem: 9852\n",
            "Epoch: [26]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.4315 (0.4598)  loss_classifier: 0.0071 (0.0077)  loss_box_reg: 0.0037 (0.0063)  loss_mask: 0.1106 (0.1300)  loss_objectness: 0.0277 (0.0333)  loss_rpn_box_reg: 0.2738 (0.2826)  time: 1.9065  data: 0.0881  max mem: 9852\n",
            "Epoch: [26] Total time: 0:01:54 (1.9006 s / it)\n",
            "Epoch: 26 Validation IoU: 0.5514045141560047\n",
            "Epoch: [27]  [ 0/60]  eta: 0:01:55  lr: 0.000100  loss: 0.4578 (0.4578)  loss_classifier: 0.0068 (0.0068)  loss_box_reg: 0.0044 (0.0044)  loss_mask: 0.1067 (0.1067)  loss_objectness: 0.0192 (0.0192)  loss_rpn_box_reg: 0.3207 (0.3207)  time: 1.9211  data: 0.0272  max mem: 9852\n",
            "Epoch: [27]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.4362 (0.4467)  loss_classifier: 0.0062 (0.0078)  loss_box_reg: 0.0048 (0.0066)  loss_mask: 0.1207 (0.1289)  loss_objectness: 0.0291 (0.0314)  loss_rpn_box_reg: 0.2506 (0.2720)  time: 1.9269  data: 0.0799  max mem: 9852\n",
            "Epoch: [27] Total time: 0:01:54 (1.9030 s / it)\n",
            "Epoch: 27 Validation IoU: 0.509228773802527\n",
            "Epoch: [28]  [ 0/60]  eta: 0:01:51  lr: 0.000100  loss: 0.5920 (0.5920)  loss_classifier: 0.0065 (0.0065)  loss_box_reg: 0.0144 (0.0144)  loss_mask: 0.2218 (0.2218)  loss_objectness: 0.0183 (0.0183)  loss_rpn_box_reg: 0.3310 (0.3310)  time: 1.8523  data: 0.0758  max mem: 9852\n",
            "Epoch: [28]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3695 (0.4217)  loss_classifier: 0.0041 (0.0065)  loss_box_reg: 0.0034 (0.0057)  loss_mask: 0.1189 (0.1275)  loss_objectness: 0.0230 (0.0294)  loss_rpn_box_reg: 0.2236 (0.2526)  time: 1.8604  data: 0.0673  max mem: 9852\n",
            "Epoch: [28] Total time: 0:01:54 (1.9093 s / it)\n",
            "Epoch: 28 Validation IoU: 0.5539966163804931\n",
            "Epoch: [29]  [ 0/60]  eta: 0:02:15  lr: 0.000100  loss: 0.3979 (0.3979)  loss_classifier: 0.0074 (0.0074)  loss_box_reg: 0.0045 (0.0045)  loss_mask: 0.1442 (0.1442)  loss_objectness: 0.0179 (0.0179)  loss_rpn_box_reg: 0.2240 (0.2240)  time: 2.2524  data: 0.2278  max mem: 9852\n",
            "Epoch: [29]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.4542 (0.4264)  loss_classifier: 0.0062 (0.0075)  loss_box_reg: 0.0040 (0.0054)  loss_mask: 0.1154 (0.1258)  loss_objectness: 0.0233 (0.0267)  loss_rpn_box_reg: 0.2653 (0.2609)  time: 1.9201  data: 0.0796  max mem: 9852\n",
            "Epoch: [29] Total time: 0:01:54 (1.9138 s / it)\n",
            "Epoch: 29 Validation IoU: 0.5311493903403864\n",
            "Epoch: [30]  [ 0/60]  eta: 0:01:58  lr: 0.000100  loss: 0.6016 (0.6016)  loss_classifier: 0.0109 (0.0109)  loss_box_reg: 0.0151 (0.0151)  loss_mask: 0.1770 (0.1770)  loss_objectness: 0.0300 (0.0300)  loss_rpn_box_reg: 0.3686 (0.3686)  time: 1.9724  data: 0.0776  max mem: 9852\n",
            "Epoch: [30]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3872 (0.4131)  loss_classifier: 0.0043 (0.0065)  loss_box_reg: 0.0031 (0.0057)  loss_mask: 0.1089 (0.1261)  loss_objectness: 0.0214 (0.0280)  loss_rpn_box_reg: 0.2209 (0.2468)  time: 1.8913  data: 0.0726  max mem: 9852\n",
            "Epoch: [30] Total time: 0:01:53 (1.8837 s / it)\n",
            "Epoch: 30 Validation IoU: 0.5550410435168226\n",
            "Epoch: [31]  [ 0/60]  eta: 0:01:42  lr: 0.000100  loss: 0.2406 (0.2406)  loss_classifier: 0.0023 (0.0023)  loss_box_reg: 0.0010 (0.0010)  loss_mask: 0.1044 (0.1044)  loss_objectness: 0.0329 (0.0329)  loss_rpn_box_reg: 0.1001 (0.1001)  time: 1.7011  data: 0.1133  max mem: 9852\n",
            "Epoch: [31]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3531 (0.3976)  loss_classifier: 0.0058 (0.0079)  loss_box_reg: 0.0029 (0.0052)  loss_mask: 0.1058 (0.1210)  loss_objectness: 0.0207 (0.0251)  loss_rpn_box_reg: 0.2114 (0.2384)  time: 1.8893  data: 0.0791  max mem: 9852\n",
            "Epoch: [31] Total time: 0:01:53 (1.8950 s / it)\n",
            "Epoch: 31 Validation IoU: 0.5016050907392866\n",
            "Epoch: [32]  [ 0/60]  eta: 0:01:41  lr: 0.000100  loss: 0.3107 (0.3107)  loss_classifier: 0.0009 (0.0009)  loss_box_reg: 0.0010 (0.0010)  loss_mask: 0.1042 (0.1042)  loss_objectness: 0.0129 (0.0129)  loss_rpn_box_reg: 0.1917 (0.1917)  time: 1.6906  data: 0.0852  max mem: 9852\n",
            "Epoch: [32]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3782 (0.3867)  loss_classifier: 0.0054 (0.0064)  loss_box_reg: 0.0020 (0.0049)  loss_mask: 0.1167 (0.1219)  loss_objectness: 0.0197 (0.0251)  loss_rpn_box_reg: 0.2299 (0.2285)  time: 1.9493  data: 0.0940  max mem: 9852\n",
            "Epoch: [32] Total time: 0:01:54 (1.9082 s / it)\n",
            "Epoch: 32 Validation IoU: 0.5452627424239569\n",
            "Epoch: [33]  [ 0/60]  eta: 0:02:02  lr: 0.000100  loss: 0.6099 (0.6099)  loss_classifier: 0.0216 (0.0216)  loss_box_reg: 0.0128 (0.0128)  loss_mask: 0.1578 (0.1578)  loss_objectness: 0.0431 (0.0431)  loss_rpn_box_reg: 0.3746 (0.3746)  time: 2.0395  data: 0.0567  max mem: 9852\n",
            "Epoch: [33]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3411 (0.3847)  loss_classifier: 0.0052 (0.0073)  loss_box_reg: 0.0025 (0.0050)  loss_mask: 0.1037 (0.1209)  loss_objectness: 0.0205 (0.0232)  loss_rpn_box_reg: 0.1918 (0.2283)  time: 1.8180  data: 0.0649  max mem: 9852\n",
            "Epoch: [33] Total time: 0:01:52 (1.8782 s / it)\n",
            "Epoch: 33 Validation IoU: 0.566920766301537\n",
            "Epoch: [34]  [ 0/60]  eta: 0:02:06  lr: 0.000100  loss: 0.4112 (0.4112)  loss_classifier: 0.0146 (0.0146)  loss_box_reg: 0.0073 (0.0073)  loss_mask: 0.1258 (0.1258)  loss_objectness: 0.0238 (0.0238)  loss_rpn_box_reg: 0.2397 (0.2397)  time: 2.1145  data: 0.1183  max mem: 9852\n",
            "Epoch: [34]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3915 (0.3822)  loss_classifier: 0.0047 (0.0068)  loss_box_reg: 0.0026 (0.0048)  loss_mask: 0.1199 (0.1219)  loss_objectness: 0.0200 (0.0227)  loss_rpn_box_reg: 0.2270 (0.2261)  time: 1.9682  data: 0.0881  max mem: 9852\n",
            "Epoch: [34] Total time: 0:01:54 (1.9111 s / it)\n",
            "Epoch: 34 Validation IoU: 0.4827295539422577\n",
            "Epoch: [35]  [ 0/60]  eta: 0:01:59  lr: 0.000100  loss: 0.3592 (0.3592)  loss_classifier: 0.0063 (0.0063)  loss_box_reg: 0.0042 (0.0042)  loss_mask: 0.0976 (0.0976)  loss_objectness: 0.0246 (0.0246)  loss_rpn_box_reg: 0.2265 (0.2265)  time: 1.9956  data: 0.1076  max mem: 9852\n",
            "Epoch: [35]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3330 (0.3774)  loss_classifier: 0.0067 (0.0068)  loss_box_reg: 0.0030 (0.0049)  loss_mask: 0.1082 (0.1182)  loss_objectness: 0.0175 (0.0236)  loss_rpn_box_reg: 0.1934 (0.2238)  time: 1.8991  data: 0.0757  max mem: 9852\n",
            "Epoch: [35] Total time: 0:01:53 (1.8839 s / it)\n",
            "Epoch: 35 Validation IoU: 0.528777619871812\n",
            "Epoch: [36]  [ 0/60]  eta: 0:01:45  lr: 0.000100  loss: 0.5198 (0.5198)  loss_classifier: 0.0126 (0.0126)  loss_box_reg: 0.0094 (0.0094)  loss_mask: 0.1702 (0.1702)  loss_objectness: 0.0295 (0.0295)  loss_rpn_box_reg: 0.2981 (0.2981)  time: 1.7614  data: 0.0553  max mem: 9852\n",
            "Epoch: [36]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3843 (0.3741)  loss_classifier: 0.0080 (0.0078)  loss_box_reg: 0.0041 (0.0049)  loss_mask: 0.1034 (0.1174)  loss_objectness: 0.0176 (0.0194)  loss_rpn_box_reg: 0.2371 (0.2245)  time: 1.9350  data: 0.0851  max mem: 9852\n",
            "Epoch: [36] Total time: 0:01:53 (1.8945 s / it)\n",
            "Epoch: 36 Validation IoU: 0.5139206479633999\n",
            "Epoch: [37]  [ 0/60]  eta: 0:01:42  lr: 0.000100  loss: 0.5297 (0.5297)  loss_classifier: 0.0181 (0.0181)  loss_box_reg: 0.0186 (0.0186)  loss_mask: 0.1825 (0.1825)  loss_objectness: 0.0142 (0.0142)  loss_rpn_box_reg: 0.2963 (0.2963)  time: 1.7056  data: 0.0520  max mem: 9852\n",
            "Epoch: [37]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3798 (0.3730)  loss_classifier: 0.0084 (0.0080)  loss_box_reg: 0.0040 (0.0046)  loss_mask: 0.1290 (0.1195)  loss_objectness: 0.0207 (0.0225)  loss_rpn_box_reg: 0.2285 (0.2185)  time: 1.8891  data: 0.0675  max mem: 9852\n",
            "Epoch: [37] Total time: 0:01:53 (1.8998 s / it)\n",
            "Epoch: 37 Validation IoU: 0.5192975199931618\n",
            "Epoch: [38]  [ 0/60]  eta: 0:01:45  lr: 0.000100  loss: 0.3301 (0.3301)  loss_classifier: 0.0054 (0.0054)  loss_box_reg: 0.0032 (0.0032)  loss_mask: 0.1113 (0.1113)  loss_objectness: 0.0218 (0.0218)  loss_rpn_box_reg: 0.1884 (0.1884)  time: 1.7584  data: 0.0291  max mem: 9852\n",
            "Epoch: [38]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3799 (0.3577)  loss_classifier: 0.0062 (0.0061)  loss_box_reg: 0.0028 (0.0043)  loss_mask: 0.1160 (0.1186)  loss_objectness: 0.0146 (0.0196)  loss_rpn_box_reg: 0.2124 (0.2092)  time: 1.9662  data: 0.0918  max mem: 9852\n",
            "Epoch: [38] Total time: 0:01:54 (1.9125 s / it)\n",
            "Epoch: 38 Validation IoU: 0.5384111049707307\n",
            "Epoch: [39]  [ 0/60]  eta: 0:02:00  lr: 0.000100  loss: 0.3661 (0.3661)  loss_classifier: 0.0093 (0.0093)  loss_box_reg: 0.0039 (0.0039)  loss_mask: 0.1109 (0.1109)  loss_objectness: 0.0165 (0.0165)  loss_rpn_box_reg: 0.2255 (0.2255)  time: 2.0074  data: 0.1006  max mem: 9852\n",
            "Epoch: [39]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.3445 (0.3479)  loss_classifier: 0.0051 (0.0068)  loss_box_reg: 0.0023 (0.0043)  loss_mask: 0.1036 (0.1166)  loss_objectness: 0.0183 (0.0205)  loss_rpn_box_reg: 0.1859 (0.1997)  time: 1.8308  data: 0.0591  max mem: 9852\n",
            "Epoch: [39] Total time: 0:01:53 (1.8862 s / it)\n",
            "Epoch: 39 Validation IoU: 0.545063786492708\n",
            "Epoch: [40]  [ 0/60]  eta: 0:01:46  lr: 0.000010  loss: 0.2982 (0.2982)  loss_classifier: 0.0066 (0.0066)  loss_box_reg: 0.0018 (0.0018)  loss_mask: 0.1111 (0.1111)  loss_objectness: 0.0285 (0.0285)  loss_rpn_box_reg: 0.1503 (0.1503)  time: 1.7788  data: 0.0322  max mem: 9852\n",
            "Epoch: [40]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.3286 (0.3268)  loss_classifier: 0.0045 (0.0069)  loss_box_reg: 0.0027 (0.0041)  loss_mask: 0.1089 (0.1140)  loss_objectness: 0.0167 (0.0198)  loss_rpn_box_reg: 0.1992 (0.1821)  time: 1.9508  data: 0.0796  max mem: 9852\n",
            "Epoch: [40] Total time: 0:01:52 (1.8819 s / it)\n",
            "Epoch: 40 Validation IoU: 0.5375043101312658\n",
            "Epoch: [41]  [ 0/60]  eta: 0:01:46  lr: 0.000010  loss: 0.3393 (0.3393)  loss_classifier: 0.0066 (0.0066)  loss_box_reg: 0.0025 (0.0025)  loss_mask: 0.1070 (0.1070)  loss_objectness: 0.0151 (0.0151)  loss_rpn_box_reg: 0.2080 (0.2080)  time: 1.7787  data: 0.0512  max mem: 9852\n",
            "Epoch: [41]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.2878 (0.3149)  loss_classifier: 0.0055 (0.0061)  loss_box_reg: 0.0025 (0.0038)  loss_mask: 0.1096 (0.1136)  loss_objectness: 0.0132 (0.0174)  loss_rpn_box_reg: 0.1497 (0.1741)  time: 1.9602  data: 0.1070  max mem: 9852\n",
            "Epoch: [41] Total time: 0:01:53 (1.8972 s / it)\n",
            "Epoch: 41 Validation IoU: 0.5224509619284283\n",
            "Epoch: [42]  [ 0/60]  eta: 0:02:05  lr: 0.000010  loss: 0.3930 (0.3930)  loss_classifier: 0.0075 (0.0075)  loss_box_reg: 0.0075 (0.0075)  loss_mask: 0.1377 (0.1377)  loss_objectness: 0.0194 (0.0194)  loss_rpn_box_reg: 0.2210 (0.2210)  time: 2.0912  data: 0.0611  max mem: 9852\n",
            "Epoch: [42]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.2631 (0.3097)  loss_classifier: 0.0042 (0.0062)  loss_box_reg: 0.0017 (0.0036)  loss_mask: 0.1035 (0.1140)  loss_objectness: 0.0125 (0.0164)  loss_rpn_box_reg: 0.1426 (0.1695)  time: 1.8705  data: 0.0670  max mem: 9852\n",
            "Epoch: [42] Total time: 0:01:53 (1.8934 s / it)\n",
            "Epoch: 42 Validation IoU: 0.5423670064797443\n",
            "Epoch: [43]  [ 0/60]  eta: 0:01:45  lr: 0.000010  loss: 0.2660 (0.2660)  loss_classifier: 0.0037 (0.0037)  loss_box_reg: 0.0008 (0.0008)  loss_mask: 0.0916 (0.0916)  loss_objectness: 0.0084 (0.0084)  loss_rpn_box_reg: 0.1614 (0.1614)  time: 1.7567  data: 0.0234  max mem: 9852\n",
            "Epoch: [43]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.3031 (0.3091)  loss_classifier: 0.0048 (0.0061)  loss_box_reg: 0.0018 (0.0036)  loss_mask: 0.1038 (0.1139)  loss_objectness: 0.0162 (0.0175)  loss_rpn_box_reg: 0.1574 (0.1680)  time: 1.9380  data: 0.0762  max mem: 9852\n",
            "Epoch: [43] Total time: 0:01:54 (1.9146 s / it)\n",
            "Epoch: 43 Validation IoU: 0.5224062821852749\n",
            "Epoch: [44]  [ 0/60]  eta: 0:01:52  lr: 0.000010  loss: 0.3012 (0.3012)  loss_classifier: 0.0055 (0.0055)  loss_box_reg: 0.0025 (0.0025)  loss_mask: 0.0969 (0.0969)  loss_objectness: 0.0200 (0.0200)  loss_rpn_box_reg: 0.1762 (0.1762)  time: 1.8669  data: 0.0672  max mem: 9852\n",
            "Epoch: [44]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.2839 (0.3050)  loss_classifier: 0.0036 (0.0053)  loss_box_reg: 0.0014 (0.0032)  loss_mask: 0.1020 (0.1111)  loss_objectness: 0.0179 (0.0178)  loss_rpn_box_reg: 0.1552 (0.1676)  time: 1.8829  data: 0.0909  max mem: 9852\n",
            "Epoch: [44] Total time: 0:01:53 (1.8895 s / it)\n",
            "Epoch: 44 Validation IoU: 0.5312686220421504\n",
            "Epoch: [45]  [ 0/60]  eta: 0:02:27  lr: 0.000010  loss: 0.3531 (0.3531)  loss_classifier: 0.0116 (0.0116)  loss_box_reg: 0.0049 (0.0049)  loss_mask: 0.1313 (0.1313)  loss_objectness: 0.0153 (0.0153)  loss_rpn_box_reg: 0.1900 (0.1900)  time: 2.4507  data: 0.2108  max mem: 9852\n",
            "Epoch: [45]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.2983 (0.3102)  loss_classifier: 0.0054 (0.0069)  loss_box_reg: 0.0024 (0.0035)  loss_mask: 0.1099 (0.1147)  loss_objectness: 0.0145 (0.0176)  loss_rpn_box_reg: 0.1804 (0.1675)  time: 1.8404  data: 0.0609  max mem: 9852\n",
            "Epoch: [45] Total time: 0:01:53 (1.8941 s / it)\n",
            "Epoch: 45 Validation IoU: 0.5121925042553316\n",
            "Epoch: [46]  [ 0/60]  eta: 0:01:49  lr: 0.000010  loss: 0.3836 (0.3836)  loss_classifier: 0.0090 (0.0090)  loss_box_reg: 0.0042 (0.0042)  loss_mask: 0.1142 (0.1142)  loss_objectness: 0.0251 (0.0251)  loss_rpn_box_reg: 0.2311 (0.2311)  time: 1.8269  data: 0.0346  max mem: 9852\n",
            "Epoch: [46]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.2593 (0.3048)  loss_classifier: 0.0041 (0.0061)  loss_box_reg: 0.0019 (0.0033)  loss_mask: 0.1007 (0.1127)  loss_objectness: 0.0178 (0.0176)  loss_rpn_box_reg: 0.1419 (0.1652)  time: 1.9021  data: 0.0876  max mem: 9852\n",
            "Epoch: [46] Total time: 0:01:53 (1.8882 s / it)\n",
            "Epoch: 46 Validation IoU: 0.5699728231612663\n",
            "Epoch: [47]  [ 0/60]  eta: 0:01:55  lr: 0.000010  loss: 0.2829 (0.2829)  loss_classifier: 0.0077 (0.0077)  loss_box_reg: 0.0017 (0.0017)  loss_mask: 0.1009 (0.1009)  loss_objectness: 0.0239 (0.0239)  loss_rpn_box_reg: 0.1487 (0.1487)  time: 1.9301  data: 0.0962  max mem: 9852\n",
            "Epoch: [47]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.2543 (0.3021)  loss_classifier: 0.0033 (0.0056)  loss_box_reg: 0.0011 (0.0035)  loss_mask: 0.1032 (0.1148)  loss_objectness: 0.0146 (0.0177)  loss_rpn_box_reg: 0.1284 (0.1605)  time: 1.8390  data: 0.0731  max mem: 9852\n",
            "Epoch: [47] Total time: 0:01:53 (1.8949 s / it)\n",
            "Epoch: 47 Validation IoU: 0.51534289007676\n",
            "Epoch: [48]  [ 0/60]  eta: 0:02:07  lr: 0.000010  loss: 0.4143 (0.4143)  loss_classifier: 0.0069 (0.0069)  loss_box_reg: 0.0078 (0.0078)  loss_mask: 0.1462 (0.1462)  loss_objectness: 0.0221 (0.0221)  loss_rpn_box_reg: 0.2312 (0.2312)  time: 2.1274  data: 0.2041  max mem: 9852\n",
            "Epoch: [48]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.2722 (0.2980)  loss_classifier: 0.0045 (0.0060)  loss_box_reg: 0.0017 (0.0033)  loss_mask: 0.1015 (0.1128)  loss_objectness: 0.0142 (0.0173)  loss_rpn_box_reg: 0.1560 (0.1586)  time: 1.9153  data: 0.0772  max mem: 9852\n",
            "Epoch: [48] Total time: 0:01:54 (1.9166 s / it)\n",
            "Epoch: 48 Validation IoU: 0.5148896006113414\n",
            "Epoch: [49]  [ 0/60]  eta: 0:01:54  lr: 0.000010  loss: 0.4006 (0.4006)  loss_classifier: 0.0082 (0.0082)  loss_box_reg: 0.0091 (0.0091)  loss_mask: 0.1760 (0.1760)  loss_objectness: 0.0096 (0.0096)  loss_rpn_box_reg: 0.1977 (0.1977)  time: 1.9026  data: 0.1570  max mem: 9852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9c1874b53c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# use helper function to train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m61\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-fdf2cfaeecef>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mslow_weights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslow_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step_counter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-fdf2cfaeecef>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAT4PKSbFp3h",
        "colab_type": "text"
      },
      "source": [
        "It seems the model is overfitting so we store and load the model weights for the highest IoU, add `weight_decay` to our optimizer for regularization and train the model again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiqamJMVFo7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the weight with highest IoU on validation set\n",
        "model.load_state_dict(best_model_wts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYz1Oh6F055G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define new optimizer and scheduler \n",
        "base_optim = RAdam(model.parameters(), lr = 0.0001, weight_decay = 1e-3) # add wight decay\n",
        "optimizer =  Lookahead(base_optim, k=5, alpha=0.5)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,  gamma=0.1)\n",
        "# = torch.optim.lr_scheduler.OneCycleLR()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ItZucgL1gvj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9796a72e-c3a1-4619-88b2-82240f559c05"
      },
      "source": [
        "# training cell final\n",
        "BATCH_SIZE=2\n",
        "N_EPOCH = 30\n",
        "\n",
        "IoU=0\n",
        "for epoch in range(N_EPOCH):\n",
        "\n",
        "  # use helper function to train model\n",
        "  train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=61)\n",
        "\n",
        "  # update the learning rate \n",
        "  scheduler.step()\n",
        "\n",
        "  # evaluate on the validation set\n",
        "  model.eval()\n",
        "  curr_IoU = 0\n",
        "  with torch.no_grad():\n",
        "    for images, targets in val_dataloader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # get predictions\n",
        "        pred = model(images)\n",
        "\n",
        "        # post-processing to get masks with scores>0.5\n",
        "        pred_masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
        "        pred_score = list(pred[0]['scores'].detach().cpu().numpy())\n",
        "        pred_t = [pred_score.index(x) for x in pred_score if x>0.5]\n",
        "        if len(pred_t) == 0:\n",
        "          continue\n",
        "        pred_t = pred_t[-1]\n",
        "\n",
        "        pred_masks = pred_masks[:pred_t+1]\n",
        "        target_masks = targets[0]['masks'].detach().cpu().numpy()\n",
        "\n",
        "        # calculate IoU for the current batch\n",
        "        batch_IoU = calc_iou(pred_masks, target_masks)\n",
        "        curr_IoU+=batch_IoU\n",
        "\n",
        "  print(\"Epoch: {} Validation IoU: {}\"\n",
        "  .format(epoch, curr_IoU/len(val_dataset)))\n",
        "  if curr_IoU>IoU:\n",
        "      best_model_wts_1 = copy.deepcopy(model.state_dict())\n",
        "      IoU = curr_IoU\n",
        "      print('Model saved in {} epoch'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0]  [ 0/60]  eta: 0:02:08  lr: 0.000002  loss: 1.0401 (1.0401)  loss_classifier: 0.0261 (0.0261)  loss_box_reg: 0.0255 (0.0255)  loss_mask: 0.2528 (0.2528)  loss_objectness: 0.0533 (0.0533)  loss_rpn_box_reg: 0.6825 (0.6825)  time: 2.1398  data: 0.0510  max mem: 6126\n",
            "Epoch: [0]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.8351 (0.9178)  loss_classifier: 0.0134 (0.0167)  loss_box_reg: 0.0121 (0.0141)  loss_mask: 0.1684 (0.1805)  loss_objectness: 0.0595 (0.0727)  loss_rpn_box_reg: 0.5398 (0.6338)  time: 1.9102  data: 0.0754  max mem: 8395\n",
            "Epoch: [0] Total time: 0:01:54 (1.9033 s / it)\n",
            "Epoch: 0 Validation IoU: 0.6254253516285333\n",
            "Model saved in 0 epoch\n",
            "Epoch: [1]  [ 0/60]  eta: 0:02:01  lr: 0.000100  loss: 0.9784 (0.9784)  loss_classifier: 0.0300 (0.0300)  loss_box_reg: 0.0238 (0.0238)  loss_mask: 0.1859 (0.1859)  loss_objectness: 0.1041 (0.1041)  loss_rpn_box_reg: 0.6346 (0.6346)  time: 2.0210  data: 0.1589  max mem: 8395\n",
            "Epoch: [1]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.7873 (0.8242)  loss_classifier: 0.0160 (0.0170)  loss_box_reg: 0.0124 (0.0143)  loss_mask: 0.1666 (0.1757)  loss_objectness: 0.0495 (0.0636)  loss_rpn_box_reg: 0.5237 (0.5536)  time: 1.9782  data: 0.0771  max mem: 9320\n",
            "Epoch: [1] Total time: 0:01:54 (1.9089 s / it)\n",
            "Epoch: 1 Validation IoU: 0.5965402607347027\n",
            "Epoch: [2]  [ 0/60]  eta: 0:02:06  lr: 0.000100  loss: 0.8035 (0.8035)  loss_classifier: 0.0172 (0.0172)  loss_box_reg: 0.0158 (0.0158)  loss_mask: 0.1834 (0.1834)  loss_objectness: 0.0536 (0.0536)  loss_rpn_box_reg: 0.5335 (0.5335)  time: 2.1003  data: 0.0547  max mem: 9320\n",
            "Epoch: [2]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.6782 (0.7711)  loss_classifier: 0.0127 (0.0154)  loss_box_reg: 0.0097 (0.0129)  loss_mask: 0.1554 (0.1712)  loss_objectness: 0.0550 (0.0613)  loss_rpn_box_reg: 0.4559 (0.5102)  time: 1.9578  data: 0.0892  max mem: 9320\n",
            "Epoch: [2] Total time: 0:01:54 (1.9155 s / it)\n",
            "Epoch: 2 Validation IoU: 0.6328172211097021\n",
            "Model saved in 2 epoch\n",
            "Epoch: [3]  [ 0/60]  eta: 0:01:58  lr: 0.000100  loss: 0.7832 (0.7832)  loss_classifier: 0.0122 (0.0122)  loss_box_reg: 0.0125 (0.0125)  loss_mask: 0.1618 (0.1618)  loss_objectness: 0.0478 (0.0478)  loss_rpn_box_reg: 0.5490 (0.5490)  time: 1.9733  data: 0.1473  max mem: 9320\n",
            "Epoch: [3]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.6709 (0.7338)  loss_classifier: 0.0138 (0.0153)  loss_box_reg: 0.0087 (0.0140)  loss_mask: 0.1477 (0.1703)  loss_objectness: 0.0497 (0.0578)  loss_rpn_box_reg: 0.4401 (0.4764)  time: 1.8812  data: 0.0761  max mem: 9843\n",
            "Epoch: [3] Total time: 0:01:56 (1.9369 s / it)\n",
            "Epoch: 3 Validation IoU: 0.589390016273536\n",
            "Epoch: [4]  [ 0/60]  eta: 0:01:48  lr: 0.000100  loss: 0.5341 (0.5341)  loss_classifier: 0.0097 (0.0097)  loss_box_reg: 0.0057 (0.0057)  loss_mask: 0.1685 (0.1685)  loss_objectness: 0.0615 (0.0615)  loss_rpn_box_reg: 0.2887 (0.2887)  time: 1.8096  data: 0.0521  max mem: 9843\n",
            "Epoch: [4]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.7219 (0.6940)  loss_classifier: 0.0129 (0.0155)  loss_box_reg: 0.0107 (0.0125)  loss_mask: 0.1457 (0.1659)  loss_objectness: 0.0549 (0.0541)  loss_rpn_box_reg: 0.4465 (0.4460)  time: 1.9358  data: 0.0774  max mem: 9843\n",
            "Epoch: [4] Total time: 0:01:55 (1.9169 s / it)\n",
            "Epoch: 4 Validation IoU: 0.5749123179693575\n",
            "Epoch: [5]  [ 0/60]  eta: 0:01:38  lr: 0.000100  loss: 0.7645 (0.7645)  loss_classifier: 0.0103 (0.0103)  loss_box_reg: 0.0143 (0.0143)  loss_mask: 0.1944 (0.1944)  loss_objectness: 0.0230 (0.0230)  loss_rpn_box_reg: 0.5225 (0.5225)  time: 1.6491  data: 0.0228  max mem: 9843\n",
            "Epoch: [5]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.6534 (0.6586)  loss_classifier: 0.0129 (0.0132)  loss_box_reg: 0.0080 (0.0108)  loss_mask: 0.1503 (0.1625)  loss_objectness: 0.0507 (0.0525)  loss_rpn_box_reg: 0.4187 (0.4197)  time: 1.9212  data: 0.0608  max mem: 9843\n",
            "Epoch: [5] Total time: 0:01:54 (1.9161 s / it)\n",
            "Epoch: 5 Validation IoU: 0.5897589057571839\n",
            "Epoch: [6]  [ 0/60]  eta: 0:02:02  lr: 0.000100  loss: 0.9041 (0.9041)  loss_classifier: 0.0289 (0.0289)  loss_box_reg: 0.0276 (0.0276)  loss_mask: 0.2278 (0.2278)  loss_objectness: 0.0754 (0.0754)  loss_rpn_box_reg: 0.5443 (0.5443)  time: 2.0443  data: 0.0545  max mem: 9843\n",
            "Epoch: [6]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.6313 (0.6418)  loss_classifier: 0.0105 (0.0136)  loss_box_reg: 0.0081 (0.0108)  loss_mask: 0.1451 (0.1601)  loss_objectness: 0.0444 (0.0527)  loss_rpn_box_reg: 0.3983 (0.4047)  time: 1.9441  data: 0.0827  max mem: 9843\n",
            "Epoch: [6] Total time: 0:01:54 (1.9120 s / it)\n",
            "Epoch: 6 Validation IoU: 0.5868544947544834\n",
            "Epoch: [7]  [ 0/60]  eta: 0:01:40  lr: 0.000100  loss: 0.3912 (0.3912)  loss_classifier: 0.0046 (0.0046)  loss_box_reg: 0.0022 (0.0022)  loss_mask: 0.1227 (0.1227)  loss_objectness: 0.0549 (0.0549)  loss_rpn_box_reg: 0.2068 (0.2068)  time: 1.6800  data: 0.0546  max mem: 9843\n",
            "Epoch: [7]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.5696 (0.6015)  loss_classifier: 0.0101 (0.0118)  loss_box_reg: 0.0047 (0.0091)  loss_mask: 0.1400 (0.1568)  loss_objectness: 0.0439 (0.0482)  loss_rpn_box_reg: 0.3183 (0.3756)  time: 1.9241  data: 0.0849  max mem: 9843\n",
            "Epoch: [7] Total time: 0:01:54 (1.9137 s / it)\n",
            "Epoch: 7 Validation IoU: 0.6436596648943038\n",
            "Model saved in 7 epoch\n",
            "Epoch: [8]  [ 0/60]  eta: 0:02:11  lr: 0.000100  loss: 0.5401 (0.5401)  loss_classifier: 0.0139 (0.0139)  loss_box_reg: 0.0062 (0.0062)  loss_mask: 0.1536 (0.1536)  loss_objectness: 0.0260 (0.0260)  loss_rpn_box_reg: 0.3405 (0.3405)  time: 2.1923  data: 0.1600  max mem: 9843\n",
            "Epoch: [8]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.5940 (0.5945)  loss_classifier: 0.0120 (0.0118)  loss_box_reg: 0.0107 (0.0095)  loss_mask: 0.1571 (0.1553)  loss_objectness: 0.0531 (0.0506)  loss_rpn_box_reg: 0.3741 (0.3673)  time: 1.9597  data: 0.0906  max mem: 9843\n",
            "Epoch: [8] Total time: 0:01:55 (1.9282 s / it)\n",
            "Epoch: 8 Validation IoU: 0.5544491747042433\n",
            "Epoch: [9]  [ 0/60]  eta: 0:01:45  lr: 0.000100  loss: 0.5290 (0.5290)  loss_classifier: 0.0092 (0.0092)  loss_box_reg: 0.0025 (0.0025)  loss_mask: 0.1072 (0.1072)  loss_objectness: 0.0363 (0.0363)  loss_rpn_box_reg: 0.3739 (0.3739)  time: 1.7580  data: 0.0495  max mem: 9843\n",
            "Epoch: [9]  [59/60]  eta: 0:00:01  lr: 0.000100  loss: 0.5534 (0.5728)  loss_classifier: 0.0100 (0.0122)  loss_box_reg: 0.0046 (0.0086)  loss_mask: 0.1347 (0.1542)  loss_objectness: 0.0334 (0.0433)  loss_rpn_box_reg: 0.3385 (0.3544)  time: 1.9370  data: 0.0786  max mem: 9843\n",
            "Epoch: [9] Total time: 0:01:55 (1.9211 s / it)\n",
            "Epoch: 9 Validation IoU: 0.6378306364834332\n",
            "Epoch: [10]  [ 0/60]  eta: 0:02:16  lr: 0.000010  loss: 0.7784 (0.7784)  loss_classifier: 0.0225 (0.0225)  loss_box_reg: 0.0333 (0.0333)  loss_mask: 0.2468 (0.2468)  loss_objectness: 0.0389 (0.0389)  loss_rpn_box_reg: 0.4369 (0.4369)  time: 2.2779  data: 0.1659  max mem: 9843\n",
            "Epoch: [10]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.4634 (0.5226)  loss_classifier: 0.0076 (0.0110)  loss_box_reg: 0.0042 (0.0075)  loss_mask: 0.1291 (0.1459)  loss_objectness: 0.0372 (0.0416)  loss_rpn_box_reg: 0.3008 (0.3167)  time: 1.8518  data: 0.0717  max mem: 9843\n",
            "Epoch: [10] Total time: 0:01:55 (1.9308 s / it)\n",
            "Epoch: 10 Validation IoU: 0.6503812101433126\n",
            "Model saved in 10 epoch\n",
            "Epoch: [11]  [ 0/60]  eta: 0:01:45  lr: 0.000010  loss: 0.5332 (0.5332)  loss_classifier: 0.0165 (0.0165)  loss_box_reg: 0.0097 (0.0097)  loss_mask: 0.1880 (0.1880)  loss_objectness: 0.0482 (0.0482)  loss_rpn_box_reg: 0.2707 (0.2707)  time: 1.7555  data: 0.0727  max mem: 9843\n",
            "Epoch: [11]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.4669 (0.5156)  loss_classifier: 0.0095 (0.0110)  loss_box_reg: 0.0055 (0.0078)  loss_mask: 0.1306 (0.1455)  loss_objectness: 0.0373 (0.0413)  loss_rpn_box_reg: 0.2876 (0.3100)  time: 1.9406  data: 0.0784  max mem: 9846\n",
            "Epoch: [11] Total time: 0:01:55 (1.9239 s / it)\n",
            "Epoch: 11 Validation IoU: 0.5896417976927474\n",
            "Epoch: [12]  [ 0/60]  eta: 0:01:43  lr: 0.000010  loss: 0.4884 (0.4884)  loss_classifier: 0.0046 (0.0046)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 0.1050 (0.1050)  loss_objectness: 0.0409 (0.0409)  loss_rpn_box_reg: 0.3365 (0.3365)  time: 1.7310  data: 0.0503  max mem: 9846\n",
            "Epoch: [12]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.4966 (0.5121)  loss_classifier: 0.0102 (0.0111)  loss_box_reg: 0.0060 (0.0072)  loss_mask: 0.1439 (0.1444)  loss_objectness: 0.0388 (0.0398)  loss_rpn_box_reg: 0.2998 (0.3096)  time: 1.9555  data: 0.1008  max mem: 9846\n",
            "Epoch: [12] Total time: 0:01:55 (1.9284 s / it)\n",
            "Epoch: 12 Validation IoU: 0.568294151492917\n",
            "Epoch: [13]  [ 0/60]  eta: 0:02:04  lr: 0.000010  loss: 0.5182 (0.5182)  loss_classifier: 0.0219 (0.0219)  loss_box_reg: 0.0104 (0.0104)  loss_mask: 0.1392 (0.1392)  loss_objectness: 0.0262 (0.0262)  loss_rpn_box_reg: 0.3205 (0.3205)  time: 2.0728  data: 0.1024  max mem: 9846\n",
            "Epoch: [13]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.5198 (0.5003)  loss_classifier: 0.0098 (0.0108)  loss_box_reg: 0.0044 (0.0068)  loss_mask: 0.1292 (0.1464)  loss_objectness: 0.0371 (0.0439)  loss_rpn_box_reg: 0.2883 (0.2925)  time: 1.8612  data: 0.0791  max mem: 9846\n",
            "Epoch: [13] Total time: 0:01:56 (1.9470 s / it)\n",
            "Epoch: 13 Validation IoU: 0.5774810220721128\n",
            "Epoch: [14]  [ 0/60]  eta: 0:02:16  lr: 0.000010  loss: 0.3768 (0.3768)  loss_classifier: 0.0083 (0.0083)  loss_box_reg: 0.0020 (0.0020)  loss_mask: 0.1128 (0.1128)  loss_objectness: 0.0400 (0.0400)  loss_rpn_box_reg: 0.2137 (0.2137)  time: 2.2740  data: 0.1827  max mem: 9846\n",
            "Epoch: [14]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.4805 (0.4871)  loss_classifier: 0.0081 (0.0094)  loss_box_reg: 0.0044 (0.0061)  loss_mask: 0.1365 (0.1414)  loss_objectness: 0.0375 (0.0380)  loss_rpn_box_reg: 0.2991 (0.2922)  time: 1.9323  data: 0.0869  max mem: 9846\n",
            "Epoch: [14] Total time: 0:01:56 (1.9378 s / it)\n",
            "Epoch: 14 Validation IoU: 0.5822391854995252\n",
            "Epoch: [15]  [ 0/60]  eta: 0:02:18  lr: 0.000010  loss: 0.4345 (0.4345)  loss_classifier: 0.0195 (0.0195)  loss_box_reg: 0.0055 (0.0055)  loss_mask: 0.1514 (0.1514)  loss_objectness: 0.0315 (0.0315)  loss_rpn_box_reg: 0.2267 (0.2267)  time: 2.3033  data: 0.2181  max mem: 9846\n",
            "Epoch: [15]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.4378 (0.4912)  loss_classifier: 0.0096 (0.0099)  loss_box_reg: 0.0057 (0.0065)  loss_mask: 0.1215 (0.1418)  loss_objectness: 0.0367 (0.0437)  loss_rpn_box_reg: 0.2922 (0.2891)  time: 1.8721  data: 0.0866  max mem: 9846\n",
            "Epoch: [15] Total time: 0:01:55 (1.9250 s / it)\n",
            "Epoch: 15 Validation IoU: 0.601480240959853\n",
            "Epoch: [16]  [ 0/60]  eta: 0:02:14  lr: 0.000010  loss: 0.4976 (0.4976)  loss_classifier: 0.0195 (0.0195)  loss_box_reg: 0.0065 (0.0065)  loss_mask: 0.1706 (0.1706)  loss_objectness: 0.0246 (0.0246)  loss_rpn_box_reg: 0.2763 (0.2763)  time: 2.2437  data: 0.1946  max mem: 9846\n",
            "Epoch: [16]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.5154 (0.4788)  loss_classifier: 0.0097 (0.0103)  loss_box_reg: 0.0049 (0.0062)  loss_mask: 0.1257 (0.1419)  loss_objectness: 0.0367 (0.0392)  loss_rpn_box_reg: 0.2885 (0.2812)  time: 1.9206  data: 0.0794  max mem: 9846\n",
            "Epoch: [16] Total time: 0:01:56 (1.9422 s / it)\n",
            "Epoch: 16 Validation IoU: 0.601126847191027\n",
            "Epoch: [17]  [ 0/60]  eta: 0:02:11  lr: 0.000010  loss: 0.4489 (0.4489)  loss_classifier: 0.0116 (0.0116)  loss_box_reg: 0.0028 (0.0028)  loss_mask: 0.1140 (0.1140)  loss_objectness: 0.0390 (0.0390)  loss_rpn_box_reg: 0.2816 (0.2816)  time: 2.1947  data: 0.1309  max mem: 9846\n",
            "Epoch: [17]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.4627 (0.4754)  loss_classifier: 0.0091 (0.0103)  loss_box_reg: 0.0035 (0.0064)  loss_mask: 0.1312 (0.1398)  loss_objectness: 0.0357 (0.0383)  loss_rpn_box_reg: 0.2708 (0.2806)  time: 1.9720  data: 0.0801  max mem: 9846\n",
            "Epoch: [17] Total time: 0:01:56 (1.9357 s / it)\n",
            "Epoch: 17 Validation IoU: 0.6063358831506429\n",
            "Epoch: [18]  [ 0/60]  eta: 0:01:52  lr: 0.000010  loss: 0.3101 (0.3101)  loss_classifier: 0.0073 (0.0073)  loss_box_reg: 0.0019 (0.0019)  loss_mask: 0.1215 (0.1215)  loss_objectness: 0.0368 (0.0368)  loss_rpn_box_reg: 0.1425 (0.1425)  time: 1.8764  data: 0.1621  max mem: 9846\n",
            "Epoch: [18]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.5171 (0.4667)  loss_classifier: 0.0077 (0.0095)  loss_box_reg: 0.0043 (0.0064)  loss_mask: 0.1346 (0.1414)  loss_objectness: 0.0391 (0.0416)  loss_rpn_box_reg: 0.2811 (0.2677)  time: 1.9506  data: 0.0906  max mem: 9846\n",
            "Epoch: [18] Total time: 0:01:55 (1.9299 s / it)\n",
            "Epoch: 18 Validation IoU: 0.5480675506947532\n",
            "Epoch: [19]  [ 0/60]  eta: 0:02:03  lr: 0.000010  loss: 0.3873 (0.3873)  loss_classifier: 0.0091 (0.0091)  loss_box_reg: 0.0043 (0.0043)  loss_mask: 0.1131 (0.1131)  loss_objectness: 0.0441 (0.0441)  loss_rpn_box_reg: 0.2168 (0.2168)  time: 2.0665  data: 0.0867  max mem: 9846\n",
            "Epoch: [19]  [59/60]  eta: 0:00:01  lr: 0.000010  loss: 0.4386 (0.4649)  loss_classifier: 0.0104 (0.0100)  loss_box_reg: 0.0044 (0.0065)  loss_mask: 0.1281 (0.1399)  loss_objectness: 0.0408 (0.0423)  loss_rpn_box_reg: 0.2584 (0.2662)  time: 1.9574  data: 0.0927  max mem: 9846\n",
            "Epoch: [19] Total time: 0:01:56 (1.9392 s / it)\n",
            "Epoch: 19 Validation IoU: 0.5568260321180768\n",
            "Epoch: [20]  [ 0/60]  eta: 0:01:53  lr: 0.000001  loss: 0.4591 (0.4591)  loss_classifier: 0.0140 (0.0140)  loss_box_reg: 0.0095 (0.0095)  loss_mask: 0.1540 (0.1540)  loss_objectness: 0.0455 (0.0455)  loss_rpn_box_reg: 0.2362 (0.2362)  time: 1.8984  data: 0.0922  max mem: 9846\n",
            "Epoch: [20]  [59/60]  eta: 0:00:01  lr: 0.000001  loss: 0.4630 (0.4604)  loss_classifier: 0.0077 (0.0092)  loss_box_reg: 0.0046 (0.0067)  loss_mask: 0.1345 (0.1413)  loss_objectness: 0.0374 (0.0383)  loss_rpn_box_reg: 0.2517 (0.2650)  time: 1.9269  data: 0.0885  max mem: 9846\n",
            "Epoch: [20] Total time: 0:01:55 (1.9296 s / it)\n",
            "Epoch: 20 Validation IoU: 0.5549741105238725\n",
            "Epoch: [21]  [ 0/60]  eta: 0:02:13  lr: 0.000001  loss: 0.4581 (0.4581)  loss_classifier: 0.0082 (0.0082)  loss_box_reg: 0.0102 (0.0102)  loss_mask: 0.1542 (0.1542)  loss_objectness: 0.0151 (0.0151)  loss_rpn_box_reg: 0.2704 (0.2704)  time: 2.2314  data: 0.1094  max mem: 9846\n",
            "Epoch: [21]  [59/60]  eta: 0:00:01  lr: 0.000001  loss: 0.3919 (0.4610)  loss_classifier: 0.0064 (0.0094)  loss_box_reg: 0.0035 (0.0065)  loss_mask: 0.1196 (0.1400)  loss_objectness: 0.0385 (0.0392)  loss_rpn_box_reg: 0.2149 (0.2659)  time: 1.8708  data: 0.0904  max mem: 9846\n",
            "Epoch: [21] Total time: 0:01:55 (1.9290 s / it)\n",
            "Epoch: 21 Validation IoU: 0.5398970174262859\n",
            "Epoch: [22]  [ 0/60]  eta: 0:01:51  lr: 0.000001  loss: 0.3999 (0.3999)  loss_classifier: 0.0090 (0.0090)  loss_box_reg: 0.0031 (0.0031)  loss_mask: 0.1251 (0.1251)  loss_objectness: 0.0462 (0.0462)  loss_rpn_box_reg: 0.2165 (0.2165)  time: 1.8639  data: 0.0702  max mem: 9846\n",
            "Epoch: [22]  [59/60]  eta: 0:00:01  lr: 0.000001  loss: 0.4112 (0.4650)  loss_classifier: 0.0062 (0.0090)  loss_box_reg: 0.0034 (0.0064)  loss_mask: 0.1299 (0.1400)  loss_objectness: 0.0353 (0.0415)  loss_rpn_box_reg: 0.2339 (0.2681)  time: 1.9148  data: 0.0882  max mem: 9846\n",
            "Epoch: [22] Total time: 0:01:56 (1.9372 s / it)\n",
            "Epoch: 22 Validation IoU: 0.5386646262483624\n",
            "Epoch: [23]  [ 0/60]  eta: 0:01:48  lr: 0.000001  loss: 0.3552 (0.3552)  loss_classifier: 0.0064 (0.0064)  loss_box_reg: 0.0025 (0.0025)  loss_mask: 0.1111 (0.1111)  loss_objectness: 0.0713 (0.0713)  loss_rpn_box_reg: 0.1639 (0.1639)  time: 1.8086  data: 0.1069  max mem: 9846\n",
            "Epoch: [23]  [59/60]  eta: 0:00:01  lr: 0.000001  loss: 0.4528 (0.4579)  loss_classifier: 0.0081 (0.0088)  loss_box_reg: 0.0045 (0.0062)  loss_mask: 0.1436 (0.1403)  loss_objectness: 0.0354 (0.0390)  loss_rpn_box_reg: 0.2516 (0.2636)  time: 1.9424  data: 0.1008  max mem: 9846\n",
            "Epoch: [23] Total time: 0:01:57 (1.9514 s / it)\n",
            "Epoch: 23 Validation IoU: 0.5936322235420022\n",
            "Epoch: [24]  [ 0/60]  eta: 0:02:09  lr: 0.000001  loss: 0.5668 (0.5668)  loss_classifier: 0.0080 (0.0080)  loss_box_reg: 0.0045 (0.0045)  loss_mask: 0.1354 (0.1354)  loss_objectness: 0.0162 (0.0162)  loss_rpn_box_reg: 0.4028 (0.4028)  time: 2.1574  data: 0.0301  max mem: 9846\n",
            "Epoch: [24]  [59/60]  eta: 0:00:01  lr: 0.000001  loss: 0.4472 (0.4627)  loss_classifier: 0.0094 (0.0097)  loss_box_reg: 0.0042 (0.0064)  loss_mask: 0.1269 (0.1389)  loss_objectness: 0.0371 (0.0401)  loss_rpn_box_reg: 0.2678 (0.2674)  time: 1.9341  data: 0.0806  max mem: 9846\n",
            "Epoch: [24] Total time: 0:01:55 (1.9269 s / it)\n",
            "Epoch: 24 Validation IoU: 0.5764221045649446\n",
            "Epoch: [25]  [ 0/60]  eta: 0:01:42  lr: 0.000001  loss: 0.5259 (0.5259)  loss_classifier: 0.0056 (0.0056)  loss_box_reg: 0.0094 (0.0094)  loss_mask: 0.1824 (0.1824)  loss_objectness: 0.0318 (0.0318)  loss_rpn_box_reg: 0.2967 (0.2967)  time: 1.7153  data: 0.0234  max mem: 9846\n",
            "Epoch: [25]  [59/60]  eta: 0:00:01  lr: 0.000001  loss: 0.4916 (0.4599)  loss_classifier: 0.0112 (0.0098)  loss_box_reg: 0.0068 (0.0063)  loss_mask: 0.1390 (0.1375)  loss_objectness: 0.0383 (0.0410)  loss_rpn_box_reg: 0.3004 (0.2654)  time: 1.9338  data: 0.0884  max mem: 9846\n",
            "Epoch: [25] Total time: 0:01:54 (1.9155 s / it)\n",
            "Epoch: 25 Validation IoU: 0.5703232712999344\n",
            "Epoch: [26]  [ 0/60]  eta: 0:02:01  lr: 0.000001  loss: 0.3625 (0.3625)  loss_classifier: 0.0035 (0.0035)  loss_box_reg: 0.0010 (0.0010)  loss_mask: 0.0923 (0.0923)  loss_objectness: 0.0231 (0.0231)  loss_rpn_box_reg: 0.2426 (0.2426)  time: 2.0168  data: 0.0482  max mem: 9846\n",
            "Epoch: [26]  [59/60]  eta: 0:00:01  lr: 0.000001  loss: 0.4357 (0.4585)  loss_classifier: 0.0048 (0.0098)  loss_box_reg: 0.0044 (0.0061)  loss_mask: 0.1326 (0.1391)  loss_objectness: 0.0383 (0.0407)  loss_rpn_box_reg: 0.2382 (0.2628)  time: 1.9194  data: 0.0991  max mem: 9846\n",
            "Epoch: [26] Total time: 0:01:55 (1.9240 s / it)\n",
            "Epoch: 26 Validation IoU: 0.5295539590609798\n",
            "Epoch: [27]  [ 0/60]  eta: 0:02:14  lr: 0.000001  loss: 0.5359 (0.5359)  loss_classifier: 0.0109 (0.0109)  loss_box_reg: 0.0116 (0.0116)  loss_mask: 0.1492 (0.1492)  loss_objectness: 0.0369 (0.0369)  loss_rpn_box_reg: 0.3273 (0.3273)  time: 2.2465  data: 0.1664  max mem: 9846\n",
            "Epoch: [27]  [59/60]  eta: 0:00:01  lr: 0.000001  loss: 0.3857 (0.4496)  loss_classifier: 0.0074 (0.0087)  loss_box_reg: 0.0039 (0.0060)  loss_mask: 0.1255 (0.1399)  loss_objectness: 0.0344 (0.0409)  loss_rpn_box_reg: 0.2155 (0.2542)  time: 1.8523  data: 0.0712  max mem: 9846\n",
            "Epoch: [27] Total time: 0:01:55 (1.9182 s / it)\n",
            "Epoch: 27 Validation IoU: 0.557819997959003\n",
            "Epoch: [28]  [ 0/60]  eta: 0:01:42  lr: 0.000001  loss: 0.3655 (0.3655)  loss_classifier: 0.0025 (0.0025)  loss_box_reg: 0.0012 (0.0012)  loss_mask: 0.1021 (0.1021)  loss_objectness: 0.0184 (0.0184)  loss_rpn_box_reg: 0.2413 (0.2413)  time: 1.7114  data: 0.0237  max mem: 9846\n",
            "Epoch: [28]  [59/60]  eta: 0:00:01  lr: 0.000001  loss: 0.4029 (0.4573)  loss_classifier: 0.0096 (0.0094)  loss_box_reg: 0.0033 (0.0062)  loss_mask: 0.1187 (0.1409)  loss_objectness: 0.0322 (0.0388)  loss_rpn_box_reg: 0.2317 (0.2620)  time: 1.9597  data: 0.0870  max mem: 9846\n",
            "Epoch: [28] Total time: 0:01:56 (1.9445 s / it)\n",
            "Epoch: 28 Validation IoU: 0.5639898302391475\n",
            "Epoch: [29]  [ 0/60]  eta: 0:01:41  lr: 0.000001  loss: 0.2768 (0.2768)  loss_classifier: 0.0039 (0.0039)  loss_box_reg: 0.0011 (0.0011)  loss_mask: 0.0984 (0.0984)  loss_objectness: 0.0176 (0.0176)  loss_rpn_box_reg: 0.1559 (0.1559)  time: 1.6910  data: 0.0424  max mem: 9846\n",
            "Epoch: [29]  [59/60]  eta: 0:00:01  lr: 0.000001  loss: 0.4325 (0.4550)  loss_classifier: 0.0083 (0.0091)  loss_box_reg: 0.0056 (0.0064)  loss_mask: 0.1313 (0.1387)  loss_objectness: 0.0411 (0.0373)  loss_rpn_box_reg: 0.2454 (0.2635)  time: 1.9086  data: 0.0779  max mem: 9846\n",
            "Epoch: [29] Total time: 0:01:55 (1.9219 s / it)\n",
            "Epoch: 29 Validation IoU: 0.5647470592734394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsDGy-d2hayH",
        "colab_type": "text"
      },
      "source": [
        "Highest IoU obtained is `0.65`. Save the model weights as shown below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtHs3wd4Gxts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = {'model': model.state_dict()}\n",
        "torch.save(checkpoint, 'maskrcnn_resnet_50.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}